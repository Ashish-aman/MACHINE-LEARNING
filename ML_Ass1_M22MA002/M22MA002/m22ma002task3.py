# -*- coding: utf-8 -*-
"""M22MA002task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ycwGVrtnwAN-hrDUezfNNA0ZhUnz_R3b

importing all the necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import numpy as np
import math
import operator
import matplotlib.pyplot as plt
# %matplotlib inline

# uplading data file from local drive in google colab
from google.colab import files
uploaded = files.upload()

import io

# reading csv file as pandas dataframe
data = pd.read_csv(io.BytesIO(uploaded['Data.csv']))
print(data)

X = data['x'].values
Y = data['y'].values

# calculate mean of x & y using an inbuilt numpy method mean()
mean_x = np.mean(X)
mean_y = np.mean(Y)

def linearReg():
  m = len(X)
  # using the formula to calculate m & c
  numer = 0
  denom = 0
  for i in range(m):
    numer += (X[i] - mean_x) * (Y[i] - mean_y)
    denom += (X[i] - mean_x) ** 2
  m = numer / denom
  c = mean_y - (m * mean_x)

  print (f'm = {m} \nc = {c}')
  # plotting values and regression line

# calculating line values x and y

  y = c + m * X
  print(y)

  plt.plot(X, y, color='#58b970', label='Regression Line') 
  plt.scatter(X, Y, c='#ef5423', label='data points')

  plt.xlabel('X')
  plt.ylabel('Y')
  plt.legend()
  plt.show()
  return 
linearReg()

#CALCULATING R2 of linear regression model
m=len(X)
num=0
denom=0
for i in range(m):
   num+=((Y[i]-y[i])**2)
   denom+=((Y[i]-mean_y)**2)
k=1-(num/denom)
print ('R2 value of data modelled using linear regression is',k)

X = data.iloc[:, 1:2].values
y = data.iloc[:, 2].values
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly_reg = PolynomialFeatures(degree=2)
X_poly = poly_reg.fit_transform(X)
pol_reg = LinearRegression()
pol_reg.fit(X_poly, y)

# Visualizing the Polymonial Regression results
def polymonialReg():
    plt.scatter(X, y, color='red')
    plt.scatter(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')
    plt.title(' (Polynomial Regression of degree 2)')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()
    return
polymonialReg()

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly_reg = PolynomialFeatures(degree=3)
X_poly = poly_reg.fit_transform(X)
pol_reg = LinearRegression()
pol_reg.fit(X_poly, y)

# Visualizing the Polymonial Regression results
def polymonialReg():
    plt.scatter(X, y, color='red')
    plt.scatter(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')
    plt.title(' (Polynomial Regression of degree 3 :OVERFIT)')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()
    return
polymonialReg()

"""HENCE ,WE HAVE SEEN FROM THE GRAPH THAT :-
1)IN LINEAR REGRESSION ,DATASET IS CUTTING AT ONLY 2 POINTS, SO ERROR RATE IS VERY HIGH.SO THE MODEL CANT BE USED FOR PREDICTION
2)IN POLYNOMIAL REGRESSION OF DEGREE 2 , ERROR RATE IS NOMINAL, AND PREDICTION WILL BE GOOD AS MAX. POINTS ARE OVERLAPPING .AND THE MODEL LOOKS GOOD
3)IN POLYNOMIAL REGRESSION OF DEGREE=3 ,CONDITION OF OVERFITTING IS CLEARLY VISIBLE AS THERE IS NO RED DOTS VISIBLE WHICH  IS ACTUAL DATA SET


HENCE POLYNOMIAL REGRESSION OF DEGREE 2 IS BEST FOR MODEL.
"""

