# -*- coding: utf-8 -*-
"""M22MAOO2_Task5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oYba_ghbpySMJbKR9svk2J2FaT-EnMjD

5.1 PLOT GRAPH OF SIGMOID FUNCTION
"""

# Import matplotlib, numpy and math
import matplotlib.pyplot as plt
import numpy as np
import math
import pandas as pd

  
x = np.linspace(-10, 10, 100)
z = 1/(1 + np.exp(-x))
  
plt.plot(x, z)
plt.xlabel("x")
plt.ylabel("Sigmoid(X)")
  
plt.show()

"""5.2 CLASSIFICATION OF WINDOW AND NON WINDOW GLASS USING MAX MIN NORMALISATION"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import math
import csv
import operator
import matplotlib.pyplot as plt
# %matplotlib inline

# uplading data file from local drive in google colab
from google.colab import files
uploaded = files.upload()

import io

# reading csv file as pandas dataframe
data = pd.read_csv(io.BytesIO(uploaded['glass.data']))
print(data)

# convert .data to csv
with open('glass.data', 'r') as read_obj, open('Glass.csv', 'w', newline='') as write_obj:
    # Create a csv.reader object from the input file object
    csv_reader = csv.reader(read_obj)
    # Create a csv.writer object from the output file object
    csv_writer = csv.writer(write_obj)
    # Read each row of the input csv file as list
    for row in csv_reader:
        # Add the updated row / list to the output file
        csv_writer.writerow(row)

data_new = pd.read_csv('Glass.csv')
print(data_new)

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
import pandas as pd



headerList = ['Id number',' RI: refractive index','Na: Sodium','Mg: Magnesium','Al: Aluminum','Si: Silicon','K: Potassium','Ca: Calcium','Ba: Barium','Fe: Iron','window_glass']

df= pd.read_csv("Glass.csv", header=None, names=headerList )
print(df)

df1=df.replace({'window_glass' : { 7 : 0, 6 : 0, 5:0}})
df1=df1.replace({'window_glass' : { 1 : 1, 2 : 1, 3:1}})


df1=df1.to_numpy()
df1

""" 5.2.2 MAXMIN NORMALISATION"""

#DEFINING MINMAX DATA SET
def dataset_minmax(datas):
	minmax = list()
	for i in range(len(datas[0])):
		col_data = [row[i] for row in datas]
		min_value = min(col_data)
		max_value = max(col_data)
		minmax.append([min_value, max_value])
	return minmax
 
# Normalizing the  dataset columns in  the range 0 to 1
def normalize_dataset(datas, minmax):
	for row in datas:
		for i in range(len(row)):
			row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])


minmax = dataset_minmax(df1)
normalize_dataset(df1, minmax)
print("the data after applying  min-max normalization")
print(df1)

df = pd.DataFrame(df1, columns = DataLabels)

x=df.drop(['window_glass'],axis=1).values
y=df['window_glass'].values

x[x.shape[1]]=1
x=np.array(x)
y=np.array(y)


# defining the cost function for the model
def cost(x_value, param,y_value):
  tcost=0
  for i in range(x_value.shape[0]):
    tcost+=(1/x_value.shape[0])*((x_value[i]*param).sum()-y_value[i])**2
  return tcost
  
#optimizing using gardient descent
def gd(x_value,y_value,param,lr,iterations):
  

  for i in range(iterations):
    slopes=np.zeros(x_value.shape[1])
    for j in range(x_value.shape[0]):
      for k in range(x_value.shape[1]):
        slopes[k]  += (1/x_value.shape[0])*((x_value[j]*param).sum()-y_value[j])*x_value[j][k]
    param=param-lr*slopes
    # print(cost(x_value,param,y_value))
  
  return param

def sigmoid(x):
  z=1/(1+np.exp(-x))
  return z

#training the model by defining prediction fn
def predict(x_train,param):
  linear_model=np.dot(x_train,param)
  y_pred=sigmoid(linear_model)
  y_pred_clss=[1 if i > 0.6 else 0 for i in y_pred]
  return y_pred_clss

def logloss(y,y_pred):
  logerror=0
  lemean=0
  for i in range(y.shape[0]):
    lemean+=(1/y.shape[0])*abs(y[i]-y_pred[i])
    logerror+=(y[i]-y_pred[i])**2
  return logerror,lemean

param=np.zeros(x.shape[1])
# y_pred=np.zeros(x_train.shape[1])

lr=0.01
iterations=1000
param=gd(x,y,param,lr,iterations)
print(param)
predict_y=predict(x,param)

print(predict_y)
# print("loss in linear reg training =",cost(x_train,param,y_train))
# print("loss in linear reg testing =",cost(x_test,param,y_test))

logerror,lemean=logloss(y,predict_y)
print("logistic_meanerror=",lemean)

"""5.3.

SPLITTING THE DATA INTO TEST AND TRAINING DATA . & APPLYING LOGISTIC REGRESSION USING GRAD ND DESC METHID
"""

df1=df.replace({'window_glass' : { 7 : 0, 6 : 0, 5:0}})
df1=df1.replace({'window_glass' : { 1 : 1, 2 : 1, 3:1}})
x=df.drop(['window_glass'],axis=1).values
y=df1['window_glass'].values
df1
y.shape[0]
df2 = df1.astype('float')


shuffle_df = df2.sample(frac=1)

train_size = int(0.6 * len(df2))

# Splitting the data in the ratio 60:40
train_set = shuffle_df[:train_size]
test_set = shuffle_df[train_size:]

x_train=train_set.drop(['window_glass'],axis=1)
x_train[x_train.shape[1]]=1
y_train=train_set['window_glass']

x_train=np.array(x_train)
y_train=np.array(y_train)


x_test=test_set.drop(['window_glass'],axis=1)
x_test[x_test.shape[1]]=1
y_test=test_set['window_glass']

x_test=np.array(x_test)
y_test=np.array(y_test)



y_train.shape[0]

"""LOGISTIC REGRESSION USING GRADIENT DESCENT METHOD"""

# cost function
def cost(x_data, params,y_data):
  total_cost=0
  for i in range(x_data.shape[0]):
    total_cost+=(1/x_data.shape[0])*((x_data[i]*params).sum()-y_data[i])**2
  return total_cost
  
#gardient descent
def gd(x_data,y_data,params,lr,iterations):
  

  for i in range(iterations):
    slopes=np.zeros(x_data.shape[1])
    for j in range(x_data.shape[0]):
      for k in range(x_data.shape[1]):
        slopes[k]  += (1/x_data.shape[0])*((x_data[j]*params).sum()-y_data[j])*x_data[j][k]
    params=params-lr*slopes
    # print(cost(x_data,params,y_data))
  
  return params

def sigmoid(x):
  z=1/(1+np.exp(-x))
  return z

def predict(x_train,param):
  linear_model=np.dot(x_train,param)
  y_pred=sigmoid(linear_model)
  y_pred_clss=[1 if i > 0.6 else 0 for i in y_pred]
  return y_pred_clss

def logloss(y,y_pred):
  logerror=0
  logerrormean=0
  for i in range(y.shape[0]):
    logerrormean+=(1/y.shape[0])*abs(y[i]-y_pred[i])
    logerror+=(y[i]-y_pred[i])**2
  return logerror,logerrormean

param=np.zeros(x.shape[1]+1)
# y_pred=np.zeros(x_train.shape[1])

lr=0.00001
iterations=2000
param=gd(x_train,y_train,param,lr,iterations)
print(param)
predict_y_train=predict(x_test,param)

print(predict_y_train)
# print("loss in linear reg training =",cost(x_train,params,y_train))
# print("loss in linear reg testing =",cost(x_test,params,y_test))

logerror,logerrormean=logloss(y_test,predict_y_train)
print("logrithimerrormean=",logerrormean)

predict_y_train=[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]

"""5.4. FIND THE CONFUSION MATRIX AND ROC DIAGRAM"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
import pandas as pd

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
import pandas as pd



headerList = ['Id number',' RI: refractive index','Na: Sodium','Mg: Magnesium','Al: Aluminum','Si: Silicon','K: Potassium','Ca: Calcium','Ba: Barium','Fe: Iron','Window glass']

df= pd.read_csv("Glass.csv", header=None, names=headerList )


df.head()

df1=df.replace({'Window glass' : { 7 : 0, 6 : 0, 5:0}})
df1=df1.replace({'Window glass' : { 1 : 1, 2 : 1, 3:1}})
x=df.drop(['Window glass'],axis=1).values
y=df1['Window glass'].values
df1
y.shape[0]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import plot_confusion_matrix
from sklearn import metrics
x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=20)

reg=LogisticRegression()
reg.fit(x_train,y_train)

metrics=plot_confusion_matrix(reg,x_test,y_test,cmap='Blues',values_format='.2g')
# conf_metrics= metrics.confusion_matrix(y_test,predict_y_train)

conf.confusion_matrix